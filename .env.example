# ============================================================================
# GrepZilla Environment Configuration
# Copy this file to .env and customize for your environment
# ============================================================================

# ----------------------------------------------------------------------------
# Application Settings
# ----------------------------------------------------------------------------

# Environment: development, staging, production
APP_ENV=development

# Enable debug mode (set to false in production)
DEBUG=true

# ----------------------------------------------------------------------------
# PostgreSQL Database
# ----------------------------------------------------------------------------

POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=grepzilla
POSTGRES_PORT=5432

# Full database URL (auto-constructed in docker-compose, set manually for local dev)
# DATABASE_URL=postgresql+asyncpg://postgres:postgres@localhost:5432/grepzilla

# Connection pool settings
DATABASE_POOL_SIZE=5
DATABASE_MAX_OVERFLOW=10

# ----------------------------------------------------------------------------
# Redis (Celery broker & result backend)
# ----------------------------------------------------------------------------

REDIS_PORT=6379
REDIS_INSIGHT_PORT=8001

# Redis URLs (auto-constructed in docker-compose, set manually for local dev)
# REDIS_URL=redis://localhost:6379/0
# REDIS_RESULT_BACKEND=redis://localhost:6379/1

# ----------------------------------------------------------------------------
# Meilisearch
# ----------------------------------------------------------------------------

MEILISEARCH_PORT=7700
MEILISEARCH_API_KEY=masterKey
MEILISEARCH_INDEX_PREFIX=grepzilla

# Meilisearch environment: development or production
MEILI_ENV=development

# Meilisearch URL (auto-constructed in docker-compose, set manually for local dev)
# MEILISEARCH_URL=http://localhost:7700

# ----------------------------------------------------------------------------
# JWT Authentication
# ----------------------------------------------------------------------------

# IMPORTANT: Change this in production!
JWT_SECRET_KEY=change-me-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# ----------------------------------------------------------------------------
# Git Provider
# ----------------------------------------------------------------------------

# Git provider type: github, gitlab, bitbucket
GIT_PROVIDER_TYPE=github

# Personal access token for Git provider API
# GIT_PROVIDER_TOKEN=your-token-here

# Base directory for cloning repositories (must be writable by the worker)
GIT_CLONE_BASE_DIR=/tmp/grepzilla/repos

# Timeout in seconds for git clone/fetch operations (30-3600)
GIT_CLONE_TIMEOUT=300

# Shallow clone depth (leave empty for full clone)
# GIT_CLONE_DEPTH=1

# ----------------------------------------------------------------------------
# API Server
# ----------------------------------------------------------------------------

API_PORT=8000
API_WORKERS=1

# ----------------------------------------------------------------------------
# Celery Workers
# ----------------------------------------------------------------------------

# Number of concurrent worker processes
CELERY_CONCURRENCY=4

# ----------------------------------------------------------------------------
# Migration Settings
# ----------------------------------------------------------------------------

# Maximum retries for waiting on dependencies
MAX_RETRIES=30

# Retry interval in seconds
RETRY_INTERVAL=2

# ----------------------------------------------------------------------------
# LLM Settings (OpenAI-compatible API)
# ----------------------------------------------------------------------------

# Base URL for LLM API (OpenAI, Ollama, Azure, etc.)
# For OpenAI: https://api.openai.com/v1
# For Ollama: http://localhost:11434/v1
# For Azure: https://<resource>.openai.azure.com/openai/deployments/<deployment>
LLM_API_BASE_URL=https://api.openai.com/v1

# API key for LLM provider (optional for local servers like Ollama)
# LLM_API_KEY=sk-your-key-here

# Model name for chat completions
# OpenAI: gpt-4o-mini, gpt-4o, gpt-3.5-turbo
# Ollama: llama3.2, mistral, codellama
LLM_MODEL=gpt-4o-mini

# Maximum tokens for LLM response
LLM_MAX_TOKENS=2048

# Temperature for LLM responses (0.0-2.0, lower = more focused)
LLM_TEMPERATURE=0.1

# Timeout in seconds for LLM API calls
LLM_TIMEOUT=60

# ----------------------------------------------------------------------------
# Embedding Settings (OpenAI-compatible API)
# ----------------------------------------------------------------------------

# Base URL for embeddings API (defaults to LLM_API_BASE_URL if not set)
# EMBEDDING_API_BASE_URL=https://api.openai.com/v1

# API key for embeddings provider (defaults to LLM_API_KEY if not set)
# EMBEDDING_API_KEY=sk-your-key-here

# Model name for embeddings
# OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
# Ollama: nomic-embed-text, mxbai-embed-large
EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimensions (optional, model-dependent)
# EMBEDDING_DIMENSIONS=1536

# Batch size for embedding requests
EMBEDDING_BATCH_SIZE=100

# Enable/disable embedding generation (set to false for text-only search)
EMBEDDING_ENABLED=true
